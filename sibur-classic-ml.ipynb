{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install strsimpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%pylab inline\nplt.style.use(\"bmh\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install cyrtranslit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyod","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom strsimpy.levenshtein import Levenshtein\nfrom strsimpy.normalized_levenshtein import NormalizedLevenshtein\nfrom strsimpy.damerau import Damerau\nfrom strsimpy.optimal_string_alignment import OptimalStringAlignment\nfrom strsimpy.jaro_winkler import JaroWinkler\nfrom strsimpy.longest_common_subsequence import LongestCommonSubsequence\nfrom strsimpy.metric_lcs import MetricLCS\nfrom strsimpy.ngram import NGram\nfrom strsimpy.qgram import QGram\nfrom strsimpy import SIFT4\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom tqdm import tqdm\ntqdm.pandas()\nimport seaborn as sns\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import sparse\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.cluster import KMeans\nimport pycountry\nimport re\nimport cyrtranslit\nfrom pyod.models.copod import COPOD\nimport networkx as nx","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/sibur20-naming-data/train.csv', index_col=0)\ntest = pd.read_csv('/kaggle/input/sibur20-naming-data/test.csv', index_col=0)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# word preprocessing\n\nПривести к нижнему регистру, кириллицу в транслит, убрать все лишние символы."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"name_1\"] = train[\"name_1\"].str.lower()\ntrain[\"name_2\"] = train[\"name_2\"].str.lower()\n\ntest[\"name_1\"] = test[\"name_1\"].str.lower()\ntest[\"name_2\"] = test[\"name_2\"].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"name_1\"] = train[\"name_1\"].progress_apply(lambda r: cyrtranslit.to_latin(r, 'ru'))\ntrain[\"name_2\"] = train[\"name_2\"].progress_apply(lambda r: cyrtranslit.to_latin(r, 'ru'))\n\ntest[\"name_1\"] = train[\"name_1\"].progress_apply(lambda r: cyrtranslit.to_latin(r, 'ru'))\ntest[\"name_2\"] = train[\"name_2\"].progress_apply(lambda r: cyrtranslit.to_latin(r, 'ru'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.replace(re.compile(r\"[^\\w\\s]\"), \"\", inplace=True)\ntest.replace(re.compile(r\"[^\\w\\s]\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train graph\n\nСформировать граф из трейна, где нодами будут слова, а ребрами - связи между соседними словами. Затем удалить из трейна и теста ноды с числом ребер, больших чем порог."},{"metadata":{"trusted":true},"cell_type":"code","source":"G = nx.Graph()\n\ndef fill_graph(s):\n    last_word = ''\n    for i, word in enumerate(s.split()):\n        G.add_node(word)\n        if i > 0:\n            G.add_edge(word, last_word)\n        last_word = word\n        \ntrain['name_1'].progress_apply(lambda r: fill_graph(r))\ntrain['name_2'].progress_apply(lambda r: fill_graph(r))\n\nG.number_of_edges(), G.number_of_nodes()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### clear words with hight graph indexes"},{"metadata":{"trusted":true},"cell_type":"code","source":"GRAPH_THRESHOLD = 50\n\ndef graph_index(s):\n    counter = 0\n    for word in s.split():\n        try:\n            counter += len(list(G.adj[word]))\n        except KeyError:\n            pass\n    return counter\n\ndef graph_filter(s):\n    result = ''\n    for word in s.split():\n        if graph_index(word) < GRAPH_THRESHOLD:\n            result += word + ' '\n    return result.rstrip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['name_1'] = train['name_1'].progress_apply(lambda r: graph_filter(r))\ntrain['name_2'] = train['name_2'].progress_apply(lambda r: graph_filter(r))\ntest['name_1'] = test['name_1'].progress_apply(lambda r: graph_filter(r))\ntest['name_2'] = test['name_2'].progress_apply(lambda r: graph_filter(r))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"После удаления частых слов часть имен обратилась в ''. Оценим число, удалим их из трейна."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test[(test['name_1'] == '') | (test['name_2'] == '')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train[(train['name_1'] == '') | (train['name_2'] == '')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[((train['name_1'] == '') | (train['name_2'] == '')) &\n      (train['is_duplicate'] == 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[(train['name_1'] != '') & (train['name_2'] != '')]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ---------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"#legal_entities = ['ООО', 'ОАО', 'ЗАО', 'ПАО', 'ОДО']\n\n#for entity in tqdm(legal_entities):\n#    train.replace(re.compile(f\"\\W*{entity}\\W*\"), \"\", inplace=True)\n#    test.replace(re.compile(f\"\\W*{entity}\\W*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#legal_entities = [\"ltd\\.\", \"co\\.\", \"inc\\.\", \"b\\.v\\.\", \"s\\.c\\.r\\.l\\.\", \"gmbh\", \"pvt\\.\"]\n\n#for entity in tqdm(legal_entities):\n#    train.replace(re.compile(f\"\\W*{entity}\\W*\"), \"\", inplace=True)\n#    test.replace(re.compile(f\"\\W*{entity}\\W*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ухудшает скор\n#shit_words = ['sa', 's a', 'de', 'cv', 'gmb h', 'g mbh', 'llc', 's pa', 'sp a', 'spa', 'ag', 'rl', 's']\n\n#for shit_word in tqdm(shit_words):\n#    train.replace(re.compile('\\s+{}'.format(shit_word)), \"\", inplace=True)\n#    test.replace(re.compile(f\"\\s+{shit_word}\\s*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#countries = [country.name.lower() for country in pycountry.countries]\n\n#for country in tqdm(countries):\n#    train.replace(re.compile(f\"\\s+{entity}\\s*\"), \"\", inplace=True)\n#    test.replace(re.compile(f\"\\s+{entity}\\s*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('Dropped', len(train[(train['name_1'] == '') | (train['name_2'] == '')]), 'samples.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train[(train['name_1'] != '') & (train['name_2'] != '')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train[train.is_duplicate==1].sample(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['name_1'].isna().sum(), train['name_2'].isna().sum(), test['name_1'].isna().sum(), test['name_2'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature generation\n\nБОЛЬШЕ ФИЧЕЙ ДЛЯ БОГА ФИЧЕЙ!"},{"metadata":{"trusted":true},"cell_type":"code","source":"levenshtein = Levenshtein()\ntrain[\"levenstein\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_1, r.name_2), axis=1)\ntest[\"levenstein\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['levenstein'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['levenstein'], label='zeros')\nplt.title('Levenstein')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_levenshtein = NormalizedLevenshtein()\n\ntrain[\"norm_levenstein\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1, r.name_2),\n                                                axis=1)\ntest[\"norm_levenstein\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1, r.name_2),\n                                              axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['norm_levenstein'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['norm_levenstein'], label='zeros')\nplt.title('Norm levenstein')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_jaccard_sim(str1, str2): \n    a = set(str1.split()) \n    b = set(str2.split())\n    c = a.intersection(b)\n    try:\n        res = float(len(c)) / (len(a) + len(b) - len(c))\n    except ZeroDivisionError:\n        res = 0\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"jaccard\"] = train.progress_apply(lambda r: get_jaccard_sim(r.name_1, r.name_2), axis=1)\ntest[\"jaccard\"] = test.progress_apply(lambda r: get_jaccard_sim(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['jaccard'], label='ones', kde=False)\nsns.distplot(train[train['is_duplicate'] == 0]['jaccard'], label='zeros', kde=False)\nplt.title('Jaccard distance')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='char', ngram_range=(1,5))\ndef ngramm_distance(str_1, str_2):\n    vectorizer.fit([str_1 + ' ' + str_2])\n    return np.absolute(vectorizer.transform([str_1]) - vectorizer.transform([str_2])).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"ngramms\"] = train.progress_apply(lambda r: ngramm_distance(r.name_1, r.name_2), axis=1)\ntest[\"ngramms\"] = test.progress_apply(lambda r: ngramm_distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['ngramms'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['ngramms'], label='zeros')\nplt.title('Ngramm distance')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_1_in_2(str_1, str_2):\n    if str_1 == '':\n        return 0\n    word = str_1.split()[0]\n    return int(word in str_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"1_in_2\"] = train.progress_apply(lambda r: calc_1_in_2(r.name_1, r.name_2), axis=1)\ntest[\"1_in_2\"] = test.progress_apply(lambda r: calc_1_in_2(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 0]['1_in_2'], label='zeros', kde=False)\nplt.title('1st word from name_1 in name_2')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['1_in_2'], label='ones', kde=False)\nplt.title('1st word from name_1 in name_2')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"2_in_1\"] = train.progress_apply(lambda r: calc_1_in_2(r.name_2, r.name_1), axis=1)\ntest[\"2_in_1\"] = test.progress_apply(lambda r: calc_1_in_2(r.name_2, r.name_1), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['2_in_1'], label='ones', kde=False)\nplt.title('1st word from name_2 in name_1')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 0]['2_in_1'], label='zeros', kde=False)\nplt.title('1st word from name_2 in name_1')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['name_1_joined'] = train['name_1'].str.replace(' ', '')\ntrain['name_2_joined'] = train['name_2'].str.replace(' ', '')\n\ntest['name_1_joined'] = test['name_1'].str.replace(' ', '')\ntest['name_2_joined'] = test['name_2'].str.replace(' ', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"jaccard_joined\"] = train.progress_apply(lambda r: get_jaccard_sim(r.name_1_joined, r.name_2_joined), axis=1)\ntest[\"jaccard_joined\"] = test.progress_apply(lambda r: get_jaccard_sim(r.name_1_joined, r.name_2_joined), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['jaccard_joined'], label='ones', kde=False)\nplt.title('Jaccard distance on joined strings')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 0]['jaccard_joined'], label='zeros', kde=False)\nplt.title('Jaccard distance on joined strings')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"levenshtein = Levenshtein()\ntrain[\"levenstein_joined\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_1_joined, r.name_2_joined), axis=1)\ntest[\"levenstein_joined\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_1_joined, r.name_2_joined), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['levenstein_joined'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['levenstein_joined'], label='zeros')\nplt.title('Levenstein distance on joined strings')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_levenshtein = NormalizedLevenshtein()\n\ntrain[\"norm_levenstein_joined\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1_joined, r.name_2_joined),\n                                                axis=1)\ntest[\"norm_levenstein_joined\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1_joined, r.name_2_joined),\n                                              axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['norm_levenstein_joined'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['norm_levenstein_joined'], label='zeros')\nplt.title('Norm levenstein distance on joined strings')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"num_chars\"] = train.progress_apply(lambda r: np.absolute(len(r.name_1_joined) - len(r.name_2_joined)), axis=1)\ntest[\"num_chars\"] = test.progress_apply(lambda r: np.absolute(len(r.name_1_joined) - len(r.name_2_joined)), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['num_chars'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['num_chars'], label='zeros')\nplt.title('Num chars difference')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"num_words\"] = train.progress_apply(lambda r: np.absolute(len(r.name_1_joined.split()) - len(r.name_2_joined.split())), axis=1)\ntest[\"num_words\"] = test.progress_apply(lambda r: np.absolute(len(r.name_1_joined.split()) - len(r.name_2_joined.split())), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['num_words'], label='ones', kde=False)\nplt.title('Num words difference')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 0]['num_words'], label='zeros', kde=False)\nplt.title('Num words difference')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _subseq_count(str_1, str_2):\n    counter = 0\n    subs = []\n    str = ''\n    for char in str_1:\n        str += char\n        subs.append(str)\n    for s in subs:\n        if s in str_2:\n            counter += len(s)\n    return counter\n\ndef subseq_count(str_1, str_2):\n    count = 0\n    for i, s in enumerate(str_1):\n        count += _subseq_count(str_1[i:], str_2)\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"subseq_rate\"] = train.progress_apply(lambda r: _subseq_count(r.name_1_joined, r.name_2_joined) + _subseq_count(r.name_2_joined, r.name_1_joined), axis=1)\ntest[\"subseq_rate\"] = test.progress_apply(lambda r: _subseq_count(r.name_1_joined, r.name_2_joined) + _subseq_count(r.name_2_joined, r.name_1_joined), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['subseq_rate'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['subseq_rate'], label='zeros')\nplt.title('Subseq rate')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_compare(str_1, str_2):\n    count = 0\n    words = str_1.split()\n    for word in words:\n        if word in str_2:\n            count += 1\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"word_compare\"] = train.progress_apply(lambda r: word_compare(r.name_1, r.name_2), axis=1)\ntest[\"word_compare\"] = test.progress_apply(lambda r: word_compare(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['word_compare'], label='ones', kde=False)\nplt.title('Num common words')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 0]['word_compare'], label='zeros', kde=False)\nplt.title('Num common words')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### word indexes"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"sum_word_index\"] = train.progress_apply(lambda r: graph_index(r.name_1 + ' ' + r.name_2), axis=1)\ntest[\"sum_word_index\"] = test.progress_apply(lambda r: graph_index(r.name_1 + ' ' + r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['sum_word_index'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['sum_word_index'], label='zeros')\nplt.title('sum_word_index')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Demarou"},{"metadata":{"trusted":true},"cell_type":"code","source":"damerau = Damerau()\ntrain[\"damerau\"] = train.progress_apply(lambda r: damerau.distance(r.name_1, r.name_2), axis=1)\ntest[\"damerau\"] = test.progress_apply(lambda r: damerau.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['damerau'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['damerau'], label='zeros')\nplt.title('damerau')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimal String Alignment"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_string_alignment = OptimalStringAlignment()\ntrain[\"optimal_string_alignment\"] = train.progress_apply(lambda r: optimal_string_alignment.distance(r.name_1, r.name_2), axis=1)\ntest[\"optimal_string_alignment\"] = test.progress_apply(lambda r: optimal_string_alignment.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['optimal_string_alignment'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['optimal_string_alignment'], label='zeros')\nplt.title('optimal_string_alignment')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Jaro-Winkler"},{"metadata":{"trusted":true},"cell_type":"code","source":"jarowinkler = JaroWinkler()\ntrain[\"jarowinkler\"] = train.progress_apply(lambda r: jarowinkler.similarity(r.name_1, r.name_2), axis=1)\ntest[\"jarowinkler\"] = test.progress_apply(lambda r: jarowinkler.similarity(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['jarowinkler'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['jarowinkler'], label='zeros')\nplt.title('jarowinkler')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Longest Common Subsequence"},{"metadata":{"trusted":true},"cell_type":"code","source":"lcs = LongestCommonSubsequence()\ntrain[\"lcs_length\"] = train.progress_apply(lambda r: lcs.length(r.name_1, r.name_2), axis=1)\ntest[\"lcs_length\"] = test.progress_apply(lambda r: lcs.length(r.name_1, r.name_2), axis=1)\ntrain[\"lcs_distance\"] = train.progress_apply(lambda r: lcs.distance(r.name_1, r.name_2), axis=1)\ntest[\"lcs_distance\"] = test.progress_apply(lambda r: lcs.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['lcs_length'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['lcs_length'], label='zeros')\nplt.title('lcs_length')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['lcs_distance'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['lcs_distance'], label='zeros')\nplt.title('lcs_distance')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MetricLCS"},{"metadata":{"trusted":true},"cell_type":"code","source":"metric_lcs = MetricLCS()\ntrain[\"metric_lcs\"] = train.progress_apply(lambda r: metric_lcs.distance(r.name_1, r.name_2), axis=1)\ntest[\"metric_lcs\"] = test.progress_apply(lambda r: metric_lcs.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['metric_lcs'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['metric_lcs'], label='zeros')\nplt.title('metric_lcs')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### twogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"twogram = NGram(2)\ntrain[\"twogram\"] = train.progress_apply(lambda r: twogram.distance(r.name_1, r.name_2), axis=1)\ntest[\"twogram\"] = test.progress_apply(lambda r: twogram.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['twogram'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['twogram'], label='zeros')\nplt.title('twogram')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Qgram"},{"metadata":{"trusted":true},"cell_type":"code","source":"qgram = QGram(2)\ntrain[\"qgram\"] = train.progress_apply(lambda r: qgram.distance(r.name_1, r.name_2), axis=1)\ntest[\"qgram\"] = test.progress_apply(lambda r: qgram.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['qgram'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['qgram'], label='zeros')\nplt.title('qgram')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SIFT4"},{"metadata":{"trusted":true},"cell_type":"code","source":"sift = SIFT4()\ntrain[\"sift\"] = train.progress_apply(lambda r: sift.distance(r.name_1, r.name_2), axis=1)\ntest[\"sift\"] = test.progress_apply(lambda r: sift.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['sift'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['sift'], label='zeros')\nplt.title('sift')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation scheme"},{"metadata":{"trusted":true},"cell_type":"code","source":"#compare 4 chars from name_1 and name_2\nsrez = 4\ntrain['4_str'] = train['name_1'].str[:srez]\nmask = train[train['name_1'].str[:srez] == train['name_2'].str[:srez]].copy()\n\n#dict for unique values\ndd = dict(zip(train['4_str'].unique(), np.arange(len(train['4_str'].unique()))))\ntrain['4_str'] = train['4_str'].map(dd)\n#for substr in train['4_str'].value_counts():\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FOLDS = 5\n\n# array of fold categories\ncats = [[] for x in range(FOLDS)]\n# array of fold lengths\nfold_length = [0] * FOLDS\n\nfor x in train['4_str'].unique():\n    # find fold with minimum length\n    min_fold = np.argmin(fold_length)\n    num_samples = train[train['4_str'] == x]['name_1'].count()\n    \n    cats[min_fold].append(x)\n    fold_length[min_fold] += num_samples\n\nprint(f'Split complete! {FOLDS} folds in {fold_length} sizes.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cats = cats[0] + cats[1] + cats[2] + cats[3]\nval_cats = cats[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_splitted = train[train['4_str'].isin(train_cats)]\nvalid_splitted = train[train['4_str'].isin(val_cats)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pseudo-labeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(X_train_splitted, X_train, X_test, scaler, vectorizer):\n    X_train_splitted[:, 2:] = scaler.fit_transform(X_train_splitted[:, 2:])\n    X_train[:, 2:] = scaler.transform(X_train[:, 2:])\n    X_test[:, 2:] = scaler.transform(X_test[:, 2:])\n    \n    vectorizer.fit(X_train_splitted[:, 0] + ' ' + X_train_splitted[:, 1])\n    X_train_splitted_ngramms = np.absolute(vectorizer.transform(X_train_splitted[:, 0]) - vectorizer.transform(X_train_splitted[:, 1]))\n    X_train_ngramms = np.absolute(vectorizer.transform(X_train[:, 0]) - vectorizer.transform(X_train[:, 1]))\n    X_test_ngramms = np.absolute(vectorizer.transform(X_test[:, 0]) - vectorizer.transform(X_test[:, 1]))\n    \n    X_train_splitted = sparse.csr_matrix(X_train_splitted[:, 2:].astype(np.float))\n    X_train = sparse.csr_matrix(X_train[:, 2:].astype(np.float))\n    X_test = sparse.csr_matrix(X_test[:, 2:].astype(np.float))\n    \n    X_train_splitted = sparse.hstack([X_train_splitted_ngramms, X_train_splitted])\n    X_train = sparse.hstack([X_train_ngramms, X_train])\n    X_test = sparse.hstack([X_test_ngramms, X_test])\n    \n    return X_train_splitted, X_train, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exclude_columns = ['name_1', 'name_2', 'name_1_joined', 'name_2_joined', '4_str']\ncolumns = train.drop(exclude_columns + ['is_duplicate'], axis=1).columns\ncolumns_lr = ['name_1', 'name_2'] + list(columns)\n\nvectorizer = CountVectorizer(analyzer='char', ngram_range=(1,5), max_features=5000)\nscaler = StandardScaler()\n\ncolumns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### lr"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Тренируем на разделенной части тренировочного датасета, генерируем лейбл на всем.\n# Скалер и энкодер тоже тренируем на разделенной части датасета.\nX_train_splitted = train_splitted[columns_lr].values\ny_train_splitted = train_splitted['is_duplicate'].values\n\nX_test = test[columns_lr].values\nX_train = train[columns_lr].values\n\nX_train_splitted, X_train, X_test = preprocess(X_train_splitted, X_train, X_test, scaler, vectorizer)\n\nlr = LogisticRegression(random_state=42, verbose=True, class_weight='balanced', C=0.01,\n                        n_jobs=-1).fit(X_train_splitted, y_train_splitted)\n\ntrain['lr_labels'] = lr.predict_proba(X_train)[:, 1]\ntest['lr_labels'] = lr.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = train[train['4_str'].isin(val_cats)]\nsns.distplot(plot_data[plot_data['is_duplicate'] == 1]['lr_labels'], label='ones')\nsns.distplot(plot_data[plot_data['is_duplicate'] == 0]['lr_labels'], label='zeros')\nplt.title('lr_labels')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### kmeans"},{"metadata":{"trusted":true},"cell_type":"code","source":"clustering = KMeans(n_clusters=20, random_state=42).fit(X_train_splitted)\ntrain['kmeans_labels'] = clustering.predict(X_train)\ntest['kmeans_labels'] = clustering.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=train[train['4_str'].isin(val_cats)], x='kmeans_labels', hue='is_duplicate')\nplt.title('kmeans_labels')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = OneHotEncoder()\nOHE_train = enc.fit_transform(train['kmeans_labels'].values.reshape(-1, 1))\nOHE_test = enc.transform(test['kmeans_labels'].values.reshape(-1, 1))\ntrain = train.drop('kmeans_labels', axis=1)\ntest = test.drop('kmeans_labels', axis=1)\ntrain = pd.concat([train, pd.DataFrame(OHE_train.toarray(), index=train.index)], axis=1)\ntest = pd.concat([test, pd.DataFrame(OHE_test.toarray(), index=test.index)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### COPOD"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = COPOD()\nclf.fit(train_splitted[columns].values)\ntrain['COPOD'] = clf.predict(train[columns].values)\ntest['COPOD'] = clf.predict(test[columns].values)\nsns.countplot(data=train[train['4_str'].isin(val_cats)], x='COPOD', hue='is_duplicate')\nplt.title('copod labels')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"exclude_columns = ['name_1', 'name_2', 'name_1_joined', 'name_2_joined', '4_str', 'lr_labels']\ncolumns = train.drop(exclude_columns + ['is_duplicate'], axis=1).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_splitted = train[train['4_str'].isin(train_cats)]\nvalid_splitted = train[train['4_str'].isin(val_cats)]\n\nX_train = train_splitted[columns].values\ny_train = train_splitted['is_duplicate'].values\ntrain_data = Pool(X_train, y_train)\nX_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid = valid_splitted[columns].values\ny_valid = valid_splitted['is_duplicate'].values\nvalid_data = Pool(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"iterations\": 1000,\n          \"depth\": 2,\n          \"loss_function\": \"CrossEntropy\",\n          \"verbose\": False,\n          \"eval_metric\": \"F1\",\n          \"random_seed\": 42,\n          \"learning_rate\": 0.8,\n#          \"auto_class_weights\": 'Balanced',\n          \"use_best_model\": True,\n          \"l2_leaf_reg\": 100\n          }\n\nmodel = CatBoostClassifier(**params)\nmodel.fit(train_data, plot=True, eval_set=valid_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model.get_feature_importance(), index=columns).sort_values(by=0, ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_valid)\nf1_score(y_valid, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(X_valid)[:, 1]\nthresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.92]\nmetrics = []\nfor thr in thresholds:\n    labels = (preds > thr).astype(int)\n    #print('-' * 10, 'THRESHOLD =', thr, '-' * 10)\n    #print(classification_report(y_valid, labels))\n    #print()\n    metric = f1_score(y_valid, labels)\n    metrics.append(metric)\nplt.plot(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train = train[columns].values\n#y_train = train['is_duplicate'].values\n#train_data = Pool(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#params['use_best_model'] = False\n#params['iterations'] = 200\n#model = CatBoostClassifier(**params)\n#model.fit(train_data, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test[columns].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(preds, threshold=0.5, filename='submit.csv', dataset=test):\n    labels = (preds > threshold).astype(int)\n    result = pd.DataFrame({'pair_id': dataset.index,\n                           'is_duplicate': labels})\n    print(f'Число положительных классов для threshold={threshold}: {result[\"is_duplicate\"].sum()} / {result[\"is_duplicate\"].mean():.2%}')\n    result.to_csv(filename, index=False)\n    print('Done!')\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(X_test)[:, 1]\nresults = []\nfor thr in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n    filename = f'submit_{thr}.csv'\n    res = submit(preds, threshold=thr, filename=filename, dataset=test)\n    results.append(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ручная валидация\n\nЕсли ни одна пара даже близко не похожа, можно не пытаться сабмитить - скор будет ниже плинтуса"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = results[-1][results[-1]['is_duplicate'] == 1].index\ntest.loc[idx, ['name_1', 'name_2']].sample(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}