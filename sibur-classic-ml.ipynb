{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install strsimpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%pylab inline\nplt.style.use(\"bmh\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install cyrtranslit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom strsimpy.levenshtein import Levenshtein\nfrom strsimpy.normalized_levenshtein import NormalizedLevenshtein\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom tqdm import tqdm\ntqdm.pandas()\nimport seaborn as sns\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import sparse\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pycountry\nimport re\nimport cyrtranslit","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/sibur20-naming-data/train.csv', index_col=0)\ntest = pd.read_csv('/kaggle/input/sibur20-naming-data/test.csv', index_col=0)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# word preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"legal_entities = ['ООО', 'ОАО', 'ЗАО', 'ПАО', 'ОДО']\n\nfor entity in tqdm(legal_entities):\n    train.replace(re.compile(f\"\\W*{entity}\\W*\"), \"\", inplace=True)\n    test.replace(re.compile(f\"\\W*{entity}\\W*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"name_1\"] = train[\"name_1\"].str.lower()\ntrain[\"name_2\"] = train[\"name_2\"].str.lower()\n\ntest[\"name_1\"] = test[\"name_1\"].str.lower()\ntest[\"name_2\"] = test[\"name_2\"].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"legal_entities = [\"ltd\\.\", \"co\\.\", \"inc\\.\", \"b\\.v\\.\", \"s\\.c\\.r\\.l\\.\", \"gmbh\", \"pvt\\.\"]\n\nfor entity in tqdm(legal_entities):\n    train.replace(re.compile(f\"\\W*{entity}\\W*\"), \"\", inplace=True)\n    test.replace(re.compile(f\"\\W*{entity}\\W*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ухудшает скор\n#shit_words = ['sa', 's a', 'de', 'cv', 'gmb h', 'g mbh', 'llc', 's pa', 'sp a', 'spa', 'ag', 'rl', 's']\n\n#for shit_word in tqdm(shit_words):\n#    train.replace(re.compile('\\s+{}'.format(shit_word)), \"\", inplace=True)\n#    test.replace(re.compile(f\"\\s+{shit_word}\\s*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = [country.name.lower() for country in pycountry.countries]\n\nfor country in tqdm(countries):\n    train.replace(re.compile(f\"\\s+{entity}\\s*\"), \"\", inplace=True)\n    test.replace(re.compile(f\"\\s+{entity}\\s*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"name_1\"] = train[\"name_1\"].progress_apply(lambda r: cyrtranslit.to_latin(r, 'ru'))\ntrain[\"name_2\"] = train[\"name_2\"].progress_apply(lambda r: cyrtranslit.to_latin(r, 'ru'))\n\ntest[\"name_1\"] = train[\"name_1\"].progress_apply(lambda r: cyrtranslit.to_latin(r, 'ru'))\ntest[\"name_2\"] = train[\"name_2\"].progress_apply(lambda r: cyrtranslit.to_latin(r, 'ru'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.replace(re.compile(r\"\\s+\\(.*\\)\"), \"\", inplace=True)\ntest.replace(re.compile(r\"\\s+\\(.*\\)\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.replace(re.compile(r\"[^\\w\\s]\"), \"\", inplace=True)\ntest.replace(re.compile(r\"[^\\w\\s]\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.is_duplicate==1].sample(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"levenshtein = Levenshtein()\ntrain[\"levenstein\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_1, r.name_2), axis=1)\ntest[\"levenstein\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_levenshtein = NormalizedLevenshtein()\n\ntrain[\"norm_levenstein\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1, r.name_2),\n                                                axis=1)\ntest[\"norm_levenstein\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1, r.name_2),\n                                              axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_jaccard_sim(str1, str2): \n    a = set(str1.split()) \n    b = set(str2.split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"jaccard\"] = train.progress_apply(lambda r: get_jaccard_sim(r.name_1, r.name_2), axis=1)\ntest[\"jaccard\"] = test.progress_apply(lambda r: get_jaccard_sim(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='char', ngram_range=(1,5))\ndef ngramm_distance(str_1, str_2):\n    vectorizer.fit([str_1 + ' ' + str_2])\n    return np.absolute(vectorizer.transform([str_1]) - vectorizer.transform([str_2])).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"ngramms\"] = train.progress_apply(lambda r: ngramm_distance(r.name_1, r.name_2), axis=1)\ntest[\"ngramms\"] = test.progress_apply(lambda r: ngramm_distance(r.name_1, r.name_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Norm_levenstein"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['norm_levenstein'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['norm_levenstein'], label='zeros')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Levenstein"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['levenstein'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['levenstein'], label='zeros')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Jaccard"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['jaccard'], label='ones', kde=False)\nsns.distplot(train[train['is_duplicate'] == 0]['jaccard'], label='zeros', kde=False)\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ngramm distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['is_duplicate'] == 1]['ngramms'], label='ones')\nsns.distplot(train[train['is_duplicate'] == 0]['ngramms'], label='zeros')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation scheme"},{"metadata":{"trusted":true},"cell_type":"code","source":"#compare 4 chars from name_1 and name_2\nsrez = 4\ntrain['4_str'] = train['name_1'].str[:srez]\nmask = train[train['name_1'].str[:srez] == train['name_2'].str[:srez]].copy()\n\n#dict for unique values\ndd = dict(zip(train['4_str'].unique(), np.arange(len(train['4_str'].unique()))))\ntrain['4_str'] = train['4_str'].map(dd)\n#for substr in train['4_str'].value_counts():\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['levenstein', 'norm_levenstein', 'jaccard', 'ngramms']\nsplit = StratifiedShuffleSplit(1, train_size=0.8, random_state=42)\ntridx, cvidx = list(split.split(train[columns], train[\"is_duplicate\"]))[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.iloc[tridx][columns].values\ny_train = train.iloc[tridx]['is_duplicate'].values\ntrain_data = Pool(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid = train.iloc[cvidx][columns].values\ny_valid = train.iloc[cvidx]['is_duplicate'].values\nvalid_data = Pool(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"iterations\": 3000,\n#          \"depth\": 2,\n          \"loss_function\": \"CrossEntropy\",\n          \"verbose\": False,\n          \"eval_metric\": \"F1\",\n          \"random_seed\": 42,\n          \"learning_rate\": 0.8,\n#          \"auto_class_weights\": 'Balanced',\n          \"use_best_model\": True\n#          \"l2_leaf_reg\": 1e12\n          }\n\nmodel = CatBoostClassifier(**params)\nmodel.fit(train_data, plot=True, eval_set=valid_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model.get_feature_importance(), index=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_valid)\nf1_score(y_valid, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(X_valid)[:, 1]\nthresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.92]\nmetrics = []\nfor thr in thresholds:\n    labels = (preds > thr).astype(int)\n    print('-' * 10, 'THRESHOLD =', thr, '-' * 10)\n    print(classification_report(y_valid, labels))\n    print()\n    metric = f1_score(y_valid, labels)\n    metrics.append(metric)\nplt.plot(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train[columns].values\ny_train = train['is_duplicate'].values\ntrain_data = Pool(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params['use_best_model'] = False\nmodel = CatBoostClassifier(**params)\nmodel.fit(train_data, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test[columns].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(preds, threshold=0.5, filename='submit.csv', dataset=test):\n    labels = (preds > threshold).astype(int)\n    result = pd.DataFrame({'pair_id': dataset.index,\n                           'is_duplicate': labels})\n    print(f'Число положительных классов для threshold={threshold}: {result[\"is_duplicate\"].sum()} / {result[\"is_duplicate\"].mean():.2%}')\n    result.to_csv(filename, index=False)\n    print('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(X_test)[:, 1]\nfor thr in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n    filename = f'submit_{thr}.csv'\n    submit(preds, threshold=thr, filename=filename, dataset=test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(X_train, X_test, scaler, vectorizer):\n    X_train[:, 2:] = scaler.fit_transform(X_train[:, 2:])\n    X_test[:, 2:] = scaler.transform(X_test[:, 2:])\n    \n    vectorizer.fit(X_train[:, 0] + ' ' + X_train[:, 1])\n    X_train_ngramms = np.absolute(vectorizer.transform(X_train[:, 0]) - vectorizer.transform(X_train[:, 1]))\n    X_test_ngramms = np.absolute(vectorizer.transform(X_test[:, 0]) - vectorizer.transform(X_test[:, 1]))\n    \n    X_train = sparse.csr_matrix(X_train[:, 2:].astype(np.float))\n    X_test = sparse.csr_matrix(X_test[:, 2:].astype(np.float))\n    \n    X_train = sparse.hstack([X_train_ngramms, X_train])\n    X_test = sparse.hstack([X_test_ngramms, X_test])\n    \n    return X_train, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FOLDS = 3\n\nsss = StratifiedShuffleSplit(FOLDS, train_size=0.8, random_state=42)\ncolumns = ['name_1', 'name_2', 'levenstein', 'norm_levenstein', 'jaccard', 'ngramms']\nX = train[columns].values\ny = train['is_duplicate'].values\n\nvectorizer = CountVectorizer(analyzer='char', ngram_range=(1,5), max_features=5000)\nscaler = StandardScaler()\nCs = [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 5e2]\nresult = pd.DataFrame()\nfold = 1\nfor train_index, test_index in tqdm(sss.split(X, y), total=FOLDS):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    X_train, X_test = preprocess(X_train, X_test, scaler, vectorizer)\n    \n    for c in Cs:\n        lr = LogisticRegression(random_state=42, class_weight='balanced',\n                                n_jobs=-1, C=c).fit(X_train, y_train)\n        preds = lr.predict(X_test)\n        score = f1_score(y_test, preds)\n        result.loc[fold, c] = score\n    fold += 1        \n\nresult.columns.name = 'C'\nresult.index.name= 'Fold'\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['name_1', 'name_2', 'levenstein', 'norm_levenstein', 'jaccard', 'ngramms']\nX_train = train[columns].values\ny_train = train['is_duplicate'].values\n\nX_test = test.values\n\nX_train, X_test = preprocess(X_train, X_test, scaler, vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=42, verbose=True, class_weight='balanced', C=100,\n                           n_jobs=-1).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(X_test)[:, 1]\nfor thr in [0.5, 0.6, 0.7, 0.8, 0.9]:\n    filename = f'submit_{thr}.csv'\n    submit(preds, threshold=thr, filename=filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}