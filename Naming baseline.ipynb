{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%pylab inline\nplt.style.use(\"bmh\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pathlib\nimport pandas as pd\n\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom torch.utils.data import Dataset, SubsetRandomSampler\n\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RS = 7345\nnp.random.seed(42)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n#device = torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Загрузка данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = pathlib.Path(\"./\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(DATA_DIR.joinpath(\"/kaggle/input/sibur20-naming-data/train.csv\"), index_col=\"pair_id\")\ntest = pd.read_csv(DATA_DIR.joinpath(\"/kaggle/input/sibur20-naming-data/test.csv\"), index_col=\"pair_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Статистика таргета"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.is_duplicate.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.is_duplicate==1].sample(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.is_duplicate==0].sample(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Очистка данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pycountry\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = [country.name.lower() for country in pycountry.countries]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"name_1\"] = train[\"name_1\"].str.lower()\ntrain[\"name_2\"] = train[\"name_2\"].str.lower()\n\ntest[\"name_1\"] = test[\"name_1\"].str.lower()\ntest[\"name_2\"] = test[\"name_2\"].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.name_1.str.contains(\"gmbh\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"legal_entities = [\"ltd.\", \"co.\", \"inc.\", \"b.v.\", \"s.c.r.l.\", \"gmbh\", \"pvt.\"]\n\nfor entity in tqdm(legal_entities):\n    train.replace(re.compile(f\"\\s+{entity}\\s*\"), \"\", inplace=True)\n    test.replace(re.compile(f\"\\s+{entity}\\s*\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.replace(re.compile(r\"\\s+\\(.*\\)\"), \"\", inplace=True)\ntest.replace(re.compile(r\"\\s+\\(.*\\)\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for country in tqdm(countries):\n    train.replace(re.compile(country), \"\", inplace=True)\n    test.replace(re.compile(country), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.replace(re.compile(r\"[^\\w\\s]\"), \"\", inplace=True)\ntest.replace(re.compile(r\"[^\\w\\s]\"), \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['full_name'] = train['name_1'] + ' # ' + train['name_2']\ntest['full_name'] = test['name_1'] + ' # ' + test['name_2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Токенизатор"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['full_name'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(s):\n    encoded_dict = tokenizer.encode_plus(s,\n                                         add_special_tokens=True,\n                                         max_length=32,\n                                         pad_to_max_length=True,\n                                         return_attention_mask=True,\n                                         return_tensors='pt',\n                                         truncation=True)\n    return encoded_dict['input_ids'], encoded_dict['attention_mask']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Побуквенный токенизатор"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Идея простая - в качестве токена символа используем номер символа Юникода\n# Для CLS токен 99998, для SEP 99999\ndef char_tokenize(sample, length=512):\n    if len(sample) > length:\n        sample = sample[:length]\n    \n    res = np.array([0] * length)\n    att = np.array([0] * length)\n    \n    for i, char in enumerate(sample):\n        res[i + 1] = ord(char)\n        att[i + 1] = 1\n    res[0] = 99998\n    res[i + 2] = 99999\n    att[0] = 1\n    att[i + 2] = 1\n    \n    res = torch.tensor(res, dtype=torch.int64).reshape(1, -1)\n    att = torch.tensor(att, dtype=torch.int64).reshape(1, -1)\n\n    return res, att ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Датасет"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SiburDataset(Dataset):\n    \"\"\"Делаем бутстрапированную (с повторениями) выборку, сбалансированную по классам.\n    Для экономии памяти делаем токанизацию в процессе выборки сэмплов.\"\"\"\n    def __init__(self, ones, zeros, size=2048, p=0.5, tokenizer=char_tokenize):\n        \"\"\"\n        size - размер выборки, т.е. фактически размер датасета для одной эпохи.\n        ones - все положительные сэмплы датасета.\n        zeros - все отрицательные сэмплы датасета.\n        p - вероятность положительного сэмпла. 0.5 для сбалансированной выборки.\n        \"\"\"\n        self.size = size\n        self.ones = ones\n        self.zeros = zeros\n        self.ones_len = len(ones)\n        self.zeros_len = len(zeros)\n        self.p = p\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        if idx >= self.size:\n            raise StopIteration\n        \n        # Решаем, какой сэмпл выдать\n        if np.random.random() < self.p:\n            sample = self.ones[np.random.randint(0, self.ones_len)]\n            target = 1\n        else:\n            sample = self.zeros[np.random.randint(0, self.zeros_len)]\n            target = 0\n        \n        X_1, att_1 = self.tokenizer(sample[0])\n        X_2, att_2 = att = self.tokenizer(sample[1])\n\n        target = torch.tensor(target, dtype=torch.long)\n        \n        return X_1.reshape(-1), att_1.reshape(-1), X_2.reshape(-1), att_2.reshape(-1), target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SiburFullDataset(Dataset):\n    \"\"\"Полный датасет без подвыборок.\"\"\"\n    def __init__(self, data, tokenizer=char_tokenize):\n        self.data = data\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data.iloc[idx]\n        word_1 = sample['name_1']\n        word_2 = sample['name_2']\n        target = sample['is_duplicate']\n        X_1, att_1 = self.tokenizer(word_1)\n        X_2, att_2 = att = self.tokenizer(word_2)\n        target = torch.tensor(target, dtype=torch.long)\n        \n        return X_1.reshape(-1), att_1.reshape(-1), X_2.reshape(-1), att_2.reshape(-1), target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SiburPredictDataset(Dataset):\n    \"\"\"Датасет для финального предикта.\"\"\"\n    def __init__(self, data, tokenizer=char_tokenize):\n        self.data = data\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        X_1, att_1 = self.tokenizer(sample[0])\n        X_2, att_2 = att = self.tokenizer(sample[1])\n        \n        return X_1.reshape(-1), att_1.reshape(-1), X_2.reshape(-1), att_2.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#тренировочная, валидационная выборки\nsplit = StratifiedShuffleSplit(1, train_size=0.8, random_state=42)\ntridx, cvidx = list(split.split(train, train[\"is_duplicate\"]))[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trdat = train.iloc[tridx]\nvaldat = train.loc[cvidx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#columns = ['name_1', 'name_2']\n#batch_size=128\n#ones = trdat.loc[trdat['is_duplicate'] == 1, columns].values\n#zeros = trdat.loc[trdat['is_duplicate'] == 0, columns].values\n#train_dataset = SiburDataset(ones, zeros, p=0.01, size=20480, tokenizer=tokenize)\n#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#batch_size=128\n#ones = valdat.loc[valdat['is_duplicate'] == 1, columns].values\n#zeros = valdat.loc[valdat['is_duplicate'] == 0, columns].values\n#valid_dataset = SiburDataset(ones, zeros, p=0.01, size=10240, tokenizer=tokenize)\n#valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\ntrain_dataset = SiburFullDataset(trdat, tokenizer=tokenize)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n\nvalid_dataset = SiburFullDataset(valdat, tokenizer=tokenize)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверка датасета\nX_1, att_1, X_2, att_2, target = next(iter(train_loader))\nX_1.shape, att_1.shape, X_2.shape, att_2.shape, target.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Модель"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SiburBerta(torch.nn.Module):\n    \"\"\"Используем мультиязычную Берту в качестве основы, прикручиваем два\n    линейных слоя на вторую голову.\"\"\"\n  \n    def __init__(self, freeze=False):\n        super(SiburBerta, self).__init__()\n        \n        self.backbone = BertModel.from_pretrained('bert-base-multilingual-cased')\n\n        # замораживаем Берту\n        if freeze:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n        \n        self.linear_1 = torch.nn.Linear(in_features=1536, out_features=256)\n        self.linear_2 = torch.nn.Linear(in_features=256, out_features=2)\n        self.softmax = torch.nn.Softmax(dim=1)\n        self.relu = torch.nn.ReLU()\n        self.flatten = torch.nn.Flatten()\n\n    def forward(self, X_1, att_1, X_2, att_2):\n        X_1 = self.backbone(X_1, att_1)[1]\n        X_2 = self.backbone(X_2, att_2)[1]\n\n        X = torch.cat((X_1, X_2), 1)\n        X = self.flatten(X)\n       \n        X = self.linear_1(X)\n        X = self.relu(X)\n        \n        X = self.linear_2(X)\n        X = self.softmax(X)\n      \n        return X   \n    \n    \nclass NNModel():\n    \n    def __init__(self, model, metric_func=f1_score, best_model_name='_best_model.pt'):\n        \"\"\"\n        model - модель pytorch\n        metric_func - функция метрики \"больше-лучше\". \n        best_model_name - название файла для промежуточного сохранения лучшей модели\n        \"\"\"\n        self.model = model\n        self.metric_func = metric_func\n        self.best_score = 0\n        self.best_ep = 0\n        self.best_model_name = best_model_name\n        self.preds = np.array([])\n        self.target = np.array([])\n        \n    def train(self, epochs, learning_rate, weight_decay, schedule_rate, loss,\n              train_loader, validation_loader=None, early_stopping=5):\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate,\n                                          weight_decay=weight_decay)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,\n                                                         step_size=1,\n                                                         gamma=schedule_rate)\n        loss_history = []\n        train_history = []\n        val_history = []\n        stop_counter = 0\n        \n        for i in range(epochs):\n            num_ep = i + 1\n            loss, metric = self._train_epoch(train_loader, num_ep)\n            loss_history.append(loss)\n            train_history.append(metric)\n            \n            if validation_loader is not None:\n                val_metric = self.evaluate(validation_loader, num_ep)\n                val_history.append(val_metric)\n                \n                if val_metric > self.best_score:\n                    self.save(self.best_model_name)\n                    self.best_ep = num_ep\n                    self.best_score = val_metric\n                    stop_counter = 0\n                else:\n                    stop_counter += 1\n                    \n                if stop_counter >= early_stopping:\n                    print('Early stopping on validation score.')\n                    break\n                \n            self.scheduler.step()\n            \n        if self.best_score > 0:\n            self.load(self.best_model_name)\n            print(f'Best model from {self.best_ep} iteration loaded.')\n                \n        return loss_history, train_history, val_history\n \n    def _train_epoch(self, train_loader, num_ep):\n        self._clear_score()\n        self.model.train()\n        losses = []\n        with tqdm(total=len(train_loader)) as progress_bar:\n            for X_1, att_1, X_2, att_2, target in train_loader:\n                X_1 = X_1.to(device)\n                X_2 = X_2.to(device)\n                att_1 = att_1.to(device)\n                att_2 = att_2.to(device)\n                target = target.to(device)\n                \n                preds = self.model(X_1, att_1, X_2, att_2)\n                loss_val = loss(preds, target)\n                \n                self.optimizer.zero_grad()\n                loss_val.backward()\n                self.optimizer.step()\n\n                losses.append(loss_val.item())\n                metric = self._score(target, preds[:, 1])\n\n                progress_bar.update()\n                progress_bar.set_description('Epoch {}: {:>5s} loss = {:.5f}, metric = {:.2f}'.format(\n                                             num_ep, 'train', np.mean(losses),  metric))\n                \n        return np.mean(losses), metric\n                \n    def _score(self, target, preds, threshold=0.5):\n        preds = preds.cpu().detach().numpy()\n        target = target.cpu().detach().numpy()\n                \n        preds = (preds > threshold).astype(int)\n        \n        self.preds = np.append(self.preds, preds)\n        self.target = np.append(self.target, target)\n        \n        metric = self.metric_func(self.target, self.preds)\n        \n        return metric\n    \n    def _clear_score(self):\n        self.preds = np.array([])\n        self.target = np.array([])\n    \n    def evaluate(self, validation_loader, num_ep):\n        self._clear_score()\n        self.model.eval()                                     \n\n        with tqdm(total=len(validation_loader)) as progress_bar:\n            with torch.no_grad():\n                for X_1, att_1, X_2, att_2, target in validation_loader:\n                    X_1 = X_1.to(device)\n                    X_2 = X_2.to(device)\n                    att_1 = att_1.to(device)\n                    att_2 = att_2.to(device)\n                    target = target.to(device)\n                \n                    preds = self.model(X_1, att_1, X_2, att_2)[:, 1]\n\n                    metric = self._score(target, preds)\n                    \n                \n                    progress_bar.update()\n                    progress_bar.set_description('Epoch {}: {:>5s} metric = {:.2f}'.format(\n                                                 num_ep, 'validation', metric))\n                \n        return metric\n    \n    def predict(self, data_loader):\n        result = np.array([])\n        self.model.eval()   \n                                                 \n        with tqdm(total=len(data_loader)) as progress_bar:\n            with torch.no_grad():\n                for X_1, att_1, X_2, att_2 in data_loader:\n                    X_1 = X_1.to(device)\n                    X_2 = X_2.to(device)\n                    att_1 = att_1.to(device)\n                    att_2 = att_2.to(device)\n\n                    preds = self.model(X_1, att_1, X_2, att_2)[:, 1]\n                    preds = preds.cpu().detach().numpy()\n                    result = np.append(result, preds)                             \n                    \n                    progress_bar.update()\n                    progress_bar.set_description('{:>5s}'.format('Prediction'))\n        return result\n    \n    def load(self, path):\n        self.model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n        return self\n        \n    def save(self, path):\n        torch.save(self.model.state_dict(), path)\n        return self\n        \n    def to(self, device):\n        self.model.to(device)\n        return self","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model = NNModel(SiburBerta(freeze=True).to(device))\nweights = torch.tensor([1, 100], dtype=torch.float32).to(device)\nloss = torch.nn.CrossEntropyLoss(weight=weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_history, train_history, val_history = model.train(epochs=20, learning_rate=5e-4, weight_decay=1e-3,\n                                                       schedule_rate=0.95, loss=loss, \n                                                       train_loader=train_loader,\n                                                       validation_loader=valid_loader, early_stopping=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#не нужно, т.к. если указан валидационный сет, то лучшая модель сохраняется сама\n#model.save('Berta_1.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_history)\nplt.title('Loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_history, label='train', c='y')\nplt.plot(val_history, label='test', c='blue')\nplt.title('Learning curves')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Сабмит"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = SiburPredictDataset(test.loc[:, ['name_1', 'name_2']].values,  tokenizer=tokenize)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# тест функции\n#preds = np.random.random(len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.load('/kaggle/input/berta-weights-sibur/_best_model.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(preds, threshold=0.5, filename='submit.csv'):\n    labels = (preds > threshold).astype(int)\n    result = pd.DataFrame({'pair_id': test.index,\n                           'is_duplicate': labels})\n    print(f'Число положительных классов для threshold={threshold}: {result[\"is_duplicate\"].sum()} / {result[\"is_duplicate\"].mean():.2%}')\n    result.to_csv(filename, index=False)\n    print('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for thr in [0.5, 0.6, 0.7, 0.8, 0.9]:\n    filename = f'submit_{thr}.csv'\n    submit(preds, threshold=thr, filename=filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}